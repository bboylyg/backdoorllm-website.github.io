<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="BackdoorLLM: A comprehensive benchmark for backdoor attacks on large language models">
  <meta name="keywords" content="BackdoorLLM, Backdoor Attacks, Large Language Models, LLM Security">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>BackdoorLLM: A Comprehensive Benchmark for Backdoor Attacks on LLMs</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/icon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/index.js"></script>
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">BackdoorLLM: A Comprehensive Benchmark for Backdoor Attacks on LLMs</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://github.com/bboylyg">Lead Author</a><sup>1</sup>,
              </span>
              <span class="author-block">Collaborators</span>
              <br />
              <span class="author-block"><sup>1</sup>Organization Name</span>
            </div>
            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/abs/xxxxx" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="ai ai-arxiv"></i></span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/bboylyg/BackdoorLLM" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fab fa-github"></i></span>
                    <span>Code</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="columns is-centered">
            <img style='height: auto; width: 100%; object-fit: contain' src="static/images/backdoorllm-cover.png" alt="BackdoorLLM Cover Image">
          </div>
        </div>
      </div> 
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="columns is-centered">
            <p>Figure: Overview of the BackdoorLLM framework for benchmarking backdoor attacks on LLMs.</p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Overview</h2>
          <div class="content has-text-justified">
            <p>BackdoorLLM is a comprehensive benchmark designed to evaluate and analyze the vulnerabilities of large language models (LLMs) to backdoor attacks. Our framework provides a systematic methodology for implanting, detecting, and mitigating backdoors in instruction-tuned and general-purpose LLMs. By leveraging diverse trigger designs and evaluation settings, BackdoorLLM offers insights into the security challenges posed by backdoor attacks and highlights potential defense mechanisms.</p>
            <p>This project introduces novel multi-trigger attack paradigms, datasets, and evaluation metrics to comprehensively assess LLM security. Our findings emphasize the importance of robust model training pipelines and data integrity checks to mitigate backdoor threats.</p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div style="text-align:center">
        <h2 class="title is-3">Method</h2>
      </div>
      <div class="content has-text-justified">
        <p>BackdoorLLM implements the following steps for conducting backdoor attacks:</p>
        <ol>
          <li>Identify candidate trigger phrases and scenarios for backdoor activation.</li>
          <li>Inject poisoned samples into the training data, ensuring stealthiness and minimal impact on model performance.</li>
          <li>Train or fine-tune LLMs on the combined clean and poisoned datasets.</li>
          <li>Evaluate the success of backdoor triggers and analyze model robustness against various defense strategies.</li>
        </ol>
        <p>Our benchmark includes both semantic and non-semantic trigger designs to assess different attack paradigms.</p>
      </div>
      <div class="columns is-centered">
        <img style='height: auto; width: 90%; object-fit: contain' src="static/images/methodology.png" alt="BackdoorLLM Methodology">
      </div>
    </div>
  </section>

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{backdoorllm2024,
  title={BackdoorLLM: A Comprehensive Benchmark for Backdoor Attacks on Large Language Models},
  author={Lead Author and Collaborators},
  journal={ArXiv},
  year={2024},
  url={https://arxiv.org/abs/xxxxx},
}</code></pre>
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="https://github.com/bboylyg/BackdoorLLM">
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              Source code is available on <a href="https://github.com/bboylyg/BackdoorLLM">GitHub</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>
</html>
