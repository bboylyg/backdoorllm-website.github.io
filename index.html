<!DOCTYPE html>
<html>

<head>
  <meta name="google-site-verification" content="LVy4NMJVYQ8yfuwcjDQx-o5hu17RhGTnl23Tb_lSZKc" />
  <meta charset="utf-8">
  <meta name="description" content="BackdoorLLM: A comprehensive benchmark for backdoor attacks on large language models">
  <meta name="keywords" content="BackdoorLLM, Backdoor Attacks, Large Language Models, LLM Security">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>BackdoorLLM: A Comprehensive Benchmark for Backdoor Attacks on LLMs</title>

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/icon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/index.js"></script>
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">BackdoorLLM: A Comprehensive Benchmark for Backdoor Attacks on LLMs</h1>
            </h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://github.com/bboylyg">
              Yige Li</a><sup>1</sup>,</span>
              <span class="author-block">
                  <a href="https://hanxunh.github.io">
              Hanxun Huang</a><sup>2</sup>,</span>
              <span class="author-block">
                  <a href="https://scholar.google.com.sg/citations?user=8Brbo6YAAAAJ&hl=en">
              Yunhan Zhao</a><sup>3</sup>,
              </span>
              <span class="author-block">
                  <a href="http://xingjunma.com">
              Xingjun Ma</a><sup>3</sup>,
              </span>
              <span class="author-block">
                  <a href="https://sunjun.site">
              Jun Sun</a><sup>1</sup>
              </span><br />
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Singapore Management University</span>
              <span class="author-block"><sup>2</sup>The University of Melbourne</span>
              <span class="author-block"><sup>3</sup>Fudan University</span>
            </div>
            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2408.12798" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="ai ai-arxiv"></i></span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/bboylyg/BackdoorLLM" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fab fa-github"></i></span>
                    <span>Code</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="columns is-centered">
             <img style='height: auto; width: 100%; object-fit: contain' src="static/images/logo.png"
              alt="overview_image">
          </div>
        </div>
      </div> 
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="columns is-centered">
            <p><b>BackdoorLLM</b> framework, the first comprehensive benchmark for studying backdoor attacks on LLMs.</p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Overview</h2>
          <div class="content has-text-justified">

            <p>We introduce <b>BackdoorLLM</b>, the first comprehensive benchmark for studying backdoor attacks on Large Language Models (LLMs). <b>BackdoorLLM</b> includes:<p>

           <ol>
          <li>- <b>A Benchmark Repository:</b> A repository of benchmarks designed to facilitate research on backdoor attacks on LLMs. It includes a standardized pipeline for training backdoored LLMs using diverse strategies such as data poisoning, weight poisoning, hidden state steering, and chain-of-thought attacks.</li>
          <li>- <b>Comprehensive Evaluations:</b> Extensive evaluations across various LLM architectures and task datasets. We evaluated six LLM models, including Llama-7B, Llama-13B, and Llama-70B, as well as other models like Mistral. Our evaluations cover backdoor attacks across representative datasets such as Stanford Alpaca, AdvBench, and math reasoning datasets, ensuring thorough assessments.</li>
          <li>- <b>Key Insights:</b> New insights into the nature of backdoor vulnerabilities in LLMs, aiding future developments in LLM backdoor defense methods.</li>
          </ol>
            <p>We hope *BackdoorLLM* can raise awareness of backdoor threats and contribute to advancing AI safety within the research community.<p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <br>
  <section class="section">
    <div class="container is-max-desktop">
      <div style="text-align:center">
        <h2 class="title is-3">Evaluation for Backdoor Attacks on LLMs</h2>
      </div><br>
      <div class="content has-text-justified">
        <p><b>BackdoorLLM</b> implements the following steps for conducting backdoor attacks:</p>
        <ol>
          <li>Identify candidate trigger phrases and scenarios for backdoor activation.</li>
          <li>Inject poisoned samples into the training data, ensuring stealthiness and minimal impact on model performance.</li>
          <li>Train or fine-tune LLMs on the combined clean and poisoned datasets.</li>
          <li>Evaluate the success of backdoor triggers and analyze model robustness against various defense strategies.</li>
        </ol>
        <p>Our benchmark includes four backdoor attack strategies, i.e. data poisoning attacks (DPA), weight poisoning attacks (WPA), hidden state attacks (HSA), and chain-of-thought attacks (CoTA) for a comprehensive benchmark, to assess different attack paradigms thoroughly.</p>
      </div>
      <br>
      <!-- 添加 GIF 演示 -->
      <div class="columns is-centered">
        <div class="column is-half has-text-centered">
          <h3 class="title is-4">Demonstration</h3>
          <img style='height: auto; width: 100%; object-fit: contain; border-radius: 10px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);' src="static/images/demo_caption.gif" alt="BackdoorLLM Demo">
          <p class="is-size-5" style="margin-top: 10px;">Demonstration Example: <b>This example shows that backdoor attacks using secret triggers can easily jailbreak well-aligned backdoored LLMs, exposing a new threat to the safe deployment of current LLMs.</b></p>
        </div>
      </div>
    </div>
  </section>

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
  @article{li2024backdoorllm,
    title={Backdoorllm: A comprehensive benchmark for backdoor attacks on large language models},
    author={Li, Yige and Huang, Hanxun and Zhao, Yunhan and Ma, Xingjun and Sun, Jun},
    journal={arXiv preprint arXiv:2408.12798},
    year={2024}
}</code></pre>
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="https://github.com/bboylyg/BackdoorLLM">
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
      
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>
</html>
